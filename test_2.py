{"cells":[{"cell_type":"markdown","metadata":{"id":"tVepTD8tblR_"},"source":["# Cat and Dog Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ww0rXZUtJoYM"},"outputs":[],"source":["from IPython.display import Image\n","# Image('./images/dog_cat.png', width=600)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GhSf8aM2-P6"},"outputs":[],"source":["\n","import torch # 파이토치 기본 라이브러리\n","\n","# torchvision : 데이터셋, 모델 아키텍처, 컴퓨터 비전의 이미지 변환 기능 제공\n","import torchvision\n","from torchvision import datasets # torchvision에서 제공하는 데이터셋\n","from torchvision import transforms # 이미지 변환기능을 제공하는 패키지\n","\n","# torch.utils.data : 파이토치 데이터 로딩 유틸리티\n","from torch.utils.data import DataLoader # 모델 훈련에 사용할 수 있는 미니 배치 구성하고\n","                                        # 매 epoch마다 데이터를 샘플링, 병렬처리 등의 일을 해주는 함수\n","\n","from torch.utils.data import random_split\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from torch.utils.tensorboard import SummaryWriter\n","from copy import deepcopy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WC6SwfAPwgLe"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1tNDFT6qxOjo"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"]},{"cell_type":"markdown","metadata":{"id":"2pXY3PNTKIjr"},"source":["# 0. 데이터 다운로드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SHuqLVoEstQQ"},"outputs":[],"source":["%pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vMt9ajPHALg9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QDB-yfkEyrHA"},"outputs":[],"source":["data_dir = '/content/drive/MyDrive/Colab Notebooks/CNN/data/'"]},{"cell_type":"markdown","metadata":{"id":"7RugCy2KwlFv"},"source":["## 1. 데이터 불러오기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-fkAbaiQa2u"},"outputs":[],"source":["# Resize: 이미지의 크기를 조절합니다.\n","# RandomResizedCrop: 이미지를 무작위로 자르고 크기를 조절합니다.\n","# RandomHorizontalFlip: 이미지를 무작위로 수평으로 뒤집습니다.\n","# RandomVerticalFlip: 이미지를 무작위로 수직으로 뒤집습니다.\n","# ToTensor: 이미지를 텐서로 변환합니다.\n","# Normalize: 이미지를 정규화합니다.\n","# ColorJitter: 이미지의 색상을 무작위로 조정합니다.\n","# RandomRotation: 이미지를 무작위로 회전합니다.\n","# RandomCrop: 이미지를 무작위로 자릅니다.\n","# Grayscale: 이미지를 흑백으로 변환합니다.\n","# RandomSizedCrop: 이미지를 무작위로 자르고 크기를 조절합니다.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nABvj80V5an4"},"outputs":[],"source":["# Compose를 통해 원하는 전처리기를 차례대로 넣을 수 있음\n","mean=(0.485, 0.456, 0.406)\n","std=(0.229, 0.224, 0.225)\n","transform = transforms.Compose([\n","                                \n","                                transforms.Resize([224, 224]),\n","                                transforms.ToTensor(),\n","\n","                                transforms.Normalize(mean = mean, std = std),\n","\n","                                transforms.RandomChoice([\n","                                        transforms.RandomHorizontalFlip(),\n","                                        transforms.RandomVerticalFlip(),\n","                                        transforms.RandomRotation(180),\n","                                 ]),\n","\n","                                \n","                                ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6md4amk0yfBw"},"outputs":[],"source":["trainset = datasets.ImageFolder(data_dir+ 'train', transform = transform)\n","testset = datasets.ImageFolder(data_dir+ 'test', transform = transform)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kOk-M8R7-_UV"},"outputs":[],"source":["print(type(trainset), len(trainset))\n","print(type(testset), len(testset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jrydbJtv_FxN"},"outputs":[],"source":["print(type(trainset.targets), len(trainset.targets), trainset.targets[:5], trainset.targets[-5:])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iOPmIdiy_0Ep"},"outputs":[],"source":["# 클래스별 분포\n","for i in range(19): # 클래스별 순회\n","  print('클래스(레이블)별 데이터 개수 : ', i, (np.array(trainset.targets) == i).sum())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v4ncx9huAujL"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","train_indices, valid_indices, _, _ = train_test_split(\n","                            range(len(trainset)), # X의 index\n","                            trainset.targets, # y\n","                            stratify=trainset.targets, # target의 비율이 train과 valid에 그대로 반영되게\n","                            test_size= 0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sNtLwPtDCEZ"},"outputs":[],"source":["len(train_indices), len(valid_indices) # 80%, 20%"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e7XrxX_XBBcj"},"outputs":[],"source":["from torch.utils.data import Subset\n","train_set = Subset(trainset, train_indices)\n","valid_set = Subset(trainset, valid_indices)\n","type(train_set), valid_set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b47ChBNbD3Q5"},"outputs":[],"source":["valid_set[0][1] # 0번째 샘플의 정답"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jtdk8KWSDZpM"},"outputs":[],"source":["# # 클래스별 분포\n","# class_list = []\n","# for i in range(19): # 클래스별 순회\n","#   s = 0\n","#   for j in range(len(valid_set)): # valid_data  순회\n","#     if valid_set[j][1] == i :\n","#       s += 1\n","#   class_list.append(s)\n","# class_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOZcw2RV0I2w"},"outputs":[],"source":["# trainset을 다시 train용과 valid 용으로 나누고자 할 때\n","# trainset, validset = random_split(trainset, [50000, 10000])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nc9aYYHaO8lP"},"outputs":[],"source":["print(type(train_set), len(train_set))\n","print(type(valid_set), len(valid_set))\n","print(type(testset), len(testset))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i6qGd00f6cDZ"},"outputs":[],"source":["# 0번째 샘플에 2개의 원소가 있는데, 그중 첫번째 원소는 이미지, 두번째 원소는 정답\n","# 그러나 파이토치로 읽어들인 이미지 텐서의 형상이 channels * height * width 임\n","# 그에 비해 opencv, matplotlib으로 읽어들인 이미지 array의 형상은 height * width * channels\n","print(train_set[1][0].size(), train_set[0][1])"]},{"cell_type":"markdown","metadata":{"id":"BM1J3AiH-oJM"},"source":["## 2. 데이터 시각화"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e4MgLZplCHfJ"},"outputs":[],"source":["labels_map = {0 : '가구수정', 1 : '걸레받이수정',2 : '곰팡이', 3 : '꼬임',4 : '녹오염',\n","              5 : '들뜸',6 : '면불량', 7 : '폴딩수정',\n","             8 : '반점', 9 : '석고수정',10 : '오염', 11 : '오타공',12 : '울음', 13 : '이음부불량',\n","              14 : '창틀,문틀수정', 15 : '터짐',\n","             16 : '틈새과다', 17 : '피스',18 : '훼손'}  # for cat and dog\n","\n","figure, axes = plt.subplots(nrows=4, ncols=8, figsize=(14, 8))\n","axes = axes.flatten()\n","\n","for i in range(32):\n","  rand_i = np.random.randint(0, len(trainset))\n","  image, label= trainset[rand_i][0].permute(1, 2, 0), trainset[rand_i][1]\n","  axes[i].axis('off')\n","  axes[i].imshow(image)\n","  axes[i].set_title(labels_map[label])"]},{"cell_type":"markdown","metadata":{"id":"m9Ue5-Gu-oJO"},"source":["## 3. 데이터 적재"]},{"cell_type":"markdown","metadata":{"id":"l9muJavJDszm"},"source":["**DataLoader**\n","- 모델 훈련에 사용할 수 있는 미니 배치 구성하고\n","- 매 epoch마다 데이터를 샘플링, 병렬처리 등의 일을 해주는 함수"]},{"cell_type":"markdown","metadata":{"id":"27oku4wuZyBt"},"source":["**배치 사이즈**\n","- 배치 사이즈 중요한 하이퍼 파라미터\n","- 16 이하로 사용하는것이 성능에 좋다고 알려져 있음\n","\n","- 배치 사이즈가 크다는 것은 실제 Loss, Gradient, Weight를 구하는 데 참여하는 데이타가 많다라는 뜻\n","- 배치 사이즈가 작을 수록 모델이 학습을 하는데 한번도 보지 않은 신선한 데이터가 제공될 확률이 큼\n","\n","- 배치 사이즈가 크면 학습시간은 줄일 수 있으나 적절한 배치사이즈로 학습을 해야 성능을 높일 수 있음\n","- (60000개의 데이터를 100개의 미니배치로 학습하면 1 epoch당 걸리는 횟수가 600번인데, 10개의 미니배치로 학습하면 1 epoch당 걸리는 횟수가 6000번)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V25Pnk8fPCa3"},"outputs":[],"source":["batch_size = 16 # 16 -> 4\n","# dataloader = DataLoader(데이터셋, 배치사이즈, 셔플여부.....)\n","trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True) # 훈련용 50000개의 데이터를 100개씩 준비\n","validloader = DataLoader(valid_set, batch_size=batch_size, shuffle=False) # 검증용 10000개의 데이터를 100개씩 준비\n","testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDfTw9WxKr5k"},"outputs":[],"source":["3111/16, 346/16, 792/16"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dcSLE4KWPCXT"},"outputs":[],"source":["print(type(trainloader), len(trainloader))\n","print(type(validloader), len(validloader))\n","print(type(testloader), len(testloader))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V9Y19MQ-FOzL"},"outputs":[],"source":["train_iter = iter(trainloader)\n","images, labels = next(train_iter)\n","images.size(), labels.size()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yzOprCd43W7R"},"outputs":[],"source":["grid_img = torchvision.utils.make_grid(images)\n","plt.imshow(grid_img.permute(1, 2, 0))"]},{"cell_type":"markdown","metadata":{"id":"O2JNaneES0VO"},"source":["**데이터 전처리기 예시(transforms)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZeqaAm81Er6o"},"outputs":[],"source":["# 샘플의 원본 이미지\n","sample_image = images[0]\n","type(sample_image), sample_image.shape"]},{"cell_type":"markdown","metadata":{"id":"mHj_UPWQeMZS"},"source":["## 4. 모델 생성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k_w6y33H4Jts"},"outputs":[],"source":["from IPython.display import Image\n","# Image('/content/전이학습.jpg', width = 600)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hNmQmL77PMWC"},"outputs":[],"source":["import torch.nn as nn # 파이토치에서 제공하는 다양한 계층 (Linear Layer, ....)\n","import torch.optim as optim # 옵티마이저 (경사하강법...)\n","import torch.nn.functional as F # 파이토치에서 제공하는 함수(활성화 함수...)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1IHyE4SIqlJR"},"outputs":[],"source":["# 가중치 초기화\n","# https://pytorch.org/docs/stable/nn.init.html\n","\n","# 현재 default 값\n","# Linear :\n","# https://github.com/pytorch/pytorch/blob/9cf62a4b5d3b287442e70c0c560a8e21d8c3b189/torch/nn/modules/linear.py#L168\n","# Conv :\n","# https://github.com/pytorch/pytorch/blob/9cf62a4b5d3b287442e70c0c560a8e21d8c3b189/torch/nn/modules/conv.py#L111"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e7m_L6kTrBXV"},"outputs":[],"source":["# 가중치 초기화시 고려할 사항\n","# 1. 값이 충분히 작아야 함\n","# 2. 값이 하나로 치우쳐선 안됨\n","# 3. 적당한 분산으로 골고루 분포가 되어야 함"]},{"cell_type":"markdown","metadata":{"id":"aTpN2VUTigSN"},"source":["**완전 연결망과 CNN망과의 차이점**\n","- 지역 연산\n","- 가중치 공유(적은 파라미터)\n","- 평행 이동 불변성"]},{"cell_type":"markdown","metadata":{"id":"EXG7RDVlYRoo"},"source":["**변경 사항**\n","- 이전 VGGNet 노트북에서는 83~84% 정확도\n","\n","**Gloabal Average Pooling**\n","- FC layer를를 없애기 위한 방법으로 GooLenet에서 도입, \n","- 최종 feature map 한장 한장에 대해 평균을 출력하여 모델 파라미터 수를 대폭 줄이게 됨\n","- 4200만개의 모델 파라미터가 Global Average Pooling으로1600만개로 줄었음\n","- 현재 노트북에서는 FC를 제거하기 전의 성능보다 좋음(90~91%)\n","\n","**Learning Rate 조정**\n","- 학습 초반에 Loss가 안정적으로 줄지 않는 현상이 있어서 0.001에서 0.0001로 변경\n","- 조정 후 Loss가 안정적으로 줄어들며 학습도 빨리 진행됨. 93~94% 로 성능 향상\n","\n","**데이터 증강(Data Augmentation)**\n","- RandomHorizontalFlip(p=0.3) 적용\n","- 모델에 입력되는 (셔플된) 미니 배치 단위로 매번 RandomHorizontalFlip이 30% 확률로 적용되므로 결국 어떤 배치에서는 원본 데이터가 그대로 사용되기도 하고 또다른 배치에서는 적용이 안되기도 해서 결국 데이터가 증강(추가)된 효과\n","- 이로 인해 모델 과적합을 막아주고 성능도 개선됨\n","- kaggle 커널에서 학습한 결과 95~96% 정확도\n","\n","**ResNet50 적용**\n","- 배치사이즈 4로 적용\n","- 93~94% 효율\n","\n","**전이학습**\n","- VGG16\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_fueMRlY5DBh"},"outputs":[],"source":["class VGG16(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.conv_block1 = nn.Sequential(\n","                                nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1),\n","                                nn.BatchNorm2d(num_features=64),\n","                                nn.ReLU(),\n","                                \n","                                nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1),\n","                                nn.BatchNorm2d(num_features=64),\n","                                nn.ReLU(),  \n","\n","                                nn.MaxPool2d(kernel_size=2, stride=2)\n","                                ) # [16, 64, 112, 112]\n","    self.conv_block2 = nn.Sequential(\n","                                nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n","                                nn.BatchNorm2d(num_features=128),                                      \n","                                nn.ReLU(),\n","\n","                                nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1),\n","                                nn.BatchNorm2d(num_features=128),                                      \n","                                nn.ReLU(),      \n","\n","                                nn.MaxPool2d(kernel_size=2, stride=2)\n","                                ) # [16, 128, 56, 56]\n","\n","    self.conv_block3 = nn.Sequential(\n","                                nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n","                                nn.BatchNorm2d(num_features=256),                                      \n","                                nn.ReLU(), \n","\n","                                nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1),\n","                                nn.BatchNorm2d(num_features=256),                                      \n","                                nn.ReLU(),   \n","\n","                                nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n","                                nn.BatchNorm2d(num_features=512),                                      \n","                                nn.ReLU(),   \n","\n","                                nn.MaxPool2d(kernel_size=2, stride=2)                                                                                         \n","                                ) # [16, 512, 28, 28]   \n","\n","    self.conv_block4 = nn.Sequential(\n","                                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n","                                nn.BatchNorm2d(num_features=512),                                      \n","                                nn.ReLU(),  \n","\n","                                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n","                                nn.BatchNorm2d(num_features=512),                                      \n","                                nn.ReLU(),  \n","\n","                                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n","                                nn.BatchNorm2d(num_features=512),                                      \n","                                nn.ReLU(),  \n","\n","                                nn.MaxPool2d(kernel_size=2, stride=2)                                                                                           \n","                                ) # [16, 512, 14, 14] \n","\n","    self.conv_block5 = nn.Sequential(\n","                                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n","                                nn.BatchNorm2d(num_features=512),                                      \n","                                nn.ReLU(),   \n","\n","                                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n","                                nn.BatchNorm2d(num_features=512),                                      \n","                                nn.ReLU(),   \n","                                \n","                                nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1),\n","                                nn.BatchNorm2d(num_features=512),                                      \n","                                nn.ReLU(),   \n","\n","                                nn.MaxPool2d(kernel_size=2, stride=2)                                   \n","                                ) # [16, 512, 7, 7]   \n","\n","    self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))  # [16, 512, 1, 1]                                                                                                                              \n","\n","    self.classifier = nn.Sequential(\n","                                nn.Linear(in_features=512, out_features=64),\n","                                nn.BatchNorm1d(num_features=64),\n","                                nn.ReLU(),\n","                                nn.Linear(in_features=64, out_features=19)\n","                                )\n","\n","  def forward(self, x):\n","    x = self.conv_block1(x) # [16, 64, 112, 112]\n","    x = self.conv_block2(x) # [16, 128, 56, 56]\n","    x = self.conv_block3(x) # [16, 512, 28, 28] \n","    x = self.conv_block4(x) # [16, 512, 14, 14]\n","    x = self.conv_block5(x) # [16, 512, 7, 7]\n","    x = self.avg_pool(x) # [16, 512, 1, 1] \n","    \n","    # reshape할 형상 : (batch_size x 512)\n","    # x = x.view(-1, 512) # option 1 : view\n","    x = torch.flatten(x, 1) # option 2 : flatten \n","    # x = x.reshape(x.shape[0], -1) # option 3 : reshape\n","\n","    x = self.classifier(x)    \n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FFFk9Ery6RTT"},"outputs":[],"source":["model = VGG16()\n","model.to(device)\n","model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yq6Zy2Y87yid"},"outputs":[],"source":["out = model(images.to(device))\n","out.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LdBSVzre5gqH"},"outputs":[],"source":["learning_rate = 0.0001\n","# 손실함수\n","loss_fn = nn.CrossEntropyLoss()\n","\n","# 옵티마이저(최적화함수, 예:경사하강법)\n","# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n","\n","# 규제의 강도 설정 weight_decay\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","# optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.001)\n","\n","# Learning Rate Schedule\n","# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html\n","\n","# 모니터링하고 있는 값(예:valid_loss)의 최소값(min) 또는 최대값(max) patience 기간동안 줄어들지 않을 때(OnPlateau) lr에 factor(0.1)를 곱해주는 전략\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4, verbose=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ALtoCmQF9Bo5"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kphlVycA8qCq"},"outputs":[],"source":["from torchsummary import summary\n","# summary(모델, (채널, 인풋사이즈))\n","summary(model, (3, 224, 224))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mr9DddpwTNPr"},"outputs":[],"source":["def validate(model, validloader, loss_fn):\n","  total = 0   \n","  correct = 0\n","  valid_loss = 0\n","  valid_accuracy = 0\n","\n","  # 전방향 예측을 구할 때는 gradient가 필요가 없음음\n","  with torch.no_grad():\n","    for images, labels in validloader: # 이터레이터로부터 next()가 호출되며 미니배치를 반환(images, labels)      \n","      # images, labels : (torch.Size([16, 3, 224, 224]), torch.Size([16]))\n","      # 0. Data를 GPU로 보내기\n","      images, labels = images.to(device), labels.to(device)\n","\n","      # 1. 입력 데이터 준비\n","      # not Flatten !!\n","      # images.resize_(images.size()[0], 784)\n","\n","      # 2. 전방향(Forward) 예측\n","      logit = model(images) # 예측 점수\n","      _, preds = torch.max(logit, 1) # 배치에 대한 최종 예측\n","      # preds = logit.max(dim=1)[1] \n","      correct += int((preds == labels).sum()) # 배치 중 맞은 것의 개수가 correct에 누적\n","      total += labels.shape[0] # 배치 사이즈만큼씩 total에 누적\n","\n","      loss = loss_fn(logit, labels)\n","      valid_loss += loss.item() # tensor에서 값을 꺼내와서, 배치의 loss 평균값을 valid_loss에 누적\n","\n","    valid_accuracy = correct / total\n","  \n","  return valid_loss, valid_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4rWxIkVqNC1h"},"outputs":[],"source":["from torch.utils.tensorboard import SummaryWriter\n","writer = SummaryWriter()\n","\n","def train_loop(model, trainloader, loss_fn, epochs, optimizer):  \n","  steps = 0\n","  steps_per_epoch = len(trainloader) \n","  min_loss = 1000000\n","  max_accuracy = 0\n","  trigger = 0\n","  patience = 7 \n","\n","  for epoch in range(epochs):\n","    model.train() # 훈련 모드\n","    train_loss = 0\n","    for images, labels in trainloader: # 이터레이터로부터 next()가 호출되며 미니배치를 반환(images, labels)\n","      steps += 1\n","      # images, labels : (torch.Size([16, 3, 224, 224]), torch.Size([16]))\n","      # 0. Data를 GPU로 보내기\n","      images, labels = images.to(device), labels.to(device)\n","\n","      # 1. 입력 데이터 준비\n","      # not Flatten !!\n","      # images.resize_(images.shape[0], 784) \n","\n","      # 2. 전방향(forward) 예측\n","      predict = model(images) # 예측 점수\n","      loss = loss_fn(predict, labels) # 예측 점수와 정답을 CrossEntropyLoss에 넣어 Loss값 반환\n","\n","      # 3. 역방향(backward) 오차(Gradient) 전파\n","      optimizer.zero_grad() # Gradient가 누적되지 않게 하기 위해\n","      loss.backward() # 모델파리미터들의 Gradient 전파\n","\n","      # 4. 경사 하강법으로 모델 파라미터 업데이트\n","      optimizer.step() # W <- W -lr*Gradient\n","\n","      train_loss += loss.item()\n","      if (steps % steps_per_epoch) == 0 : \n","        model.eval() # 평가 모드 : 평가에서 사용하지 않을 계층(배치 정규화, 드롭아웃)들을 수행하지 않게 하기 위해서\n","        valid_loss, valid_accuracy = validate(model, validloader, loss_fn)\n","\n","        # tensorboard 시각화를 위한 로그 이벤트 등록\n","        writer.add_scalar('Train Loss', train_loss/len(trainloader), epoch+1)\n","        writer.add_scalar('Valid Loss', valid_loss/len(validloader), epoch+1)\n","        writer.add_scalars('Train Loss and Valid Loss',\n","                          {'Train' : train_loss/len(trainloader),\n","                            'Valid' : valid_loss/len(validloader)}, epoch+1)\n","        writer.add_scalar('Valid Accuracy', valid_accuracy, epoch+1)\n","        # -------------------------------------------\n","\n","        print('Epoch : {}/{}.......'.format(epoch+1, epochs),            \n","              'Train Loss : {:.3f}'.format(train_loss/len(trainloader)), \n","              'Valid Loss : {:.3f}'.format(valid_loss/len(validloader)), \n","              'Valid Accuracy : {:.3f}'.format(valid_accuracy)            \n","              )\n","        \n","        # Best model 저장    \n","        # option 1 : valid_loss 모니터링\n","        # if valid_loss < min_loss: # 바로 이전 epoch의 loss보다 작으면 저장하기\n","        #   min_loss = valid_loss\n","        #   best_model_state = deepcopy(model.state_dict())          \n","        #   torch.save(best_model_state, 'best_checkpoint.pth')     \n","        \n","        # option 2 : valid_accuracy 모니터링      \n","        if valid_accuracy > max_accuracy : # 바로 이전 epoch의 accuracy보다 크면 저장하기\n","          max_accuracy = valid_accuracy\n","          best_model_state = deepcopy(model.state_dict())          \n","          torch.save(best_model_state, 'best_checkpoint.pth')  \n","        # -------------------------------------------\n","\n","        # Early Stopping (조기 종료)\n","        if valid_loss > min_loss: # valid_loss가 min_loss를 갱신하지 못하면\n","          trigger += 1\n","          print('trigger : ', trigger)\n","          if trigger > patience:\n","            print('Early Stopping !!!')\n","            print('Training loop is finished !!')\n","            writer.flush()   \n","            return\n","        else:\n","          trigger = 0\n","          min_loss = valid_loss\n","        # -------------------------------------------\n","\n","        # Learning Rate Scheduler\n","        scheduler.step(valid_loss)\n","        # -------------------------------------------\n","        \n","  writer.flush()\n","  return  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"7OqM16tTTSOr"},"outputs":[],"source":["epochs = 55\n","%time train_loop(model, trainloader, loss_fn, epochs, optimizer)\n","writer.close()"]},{"cell_type":"markdown","metadata":{"id":"Tzxn2oYIJRe5"},"source":["## 5. 모델 컴파일 (손실함수, 옵티마이저 선택)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IXcP5zCRVTt6"},"outputs":[],"source":["# Note. CrossEntropyLoss 관련\n","# https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss\n","# Note that this case is equivalent to the combination of LogSoftmax and NLLLoss.\n","# CrossEntropy를 손실함수로 사용하게 되면 forward() 계산시에 softmax() 함수를 사용하면 안됨(otherwise 중복)\n","# softmax 를 사용하면 부동 소수점 부정확성으로 인해 정확도가 떨어지고 불안정해질 수 있음\n","# forward()의 마지막 출력은 확률값이 아닌 score(logit)이어야 함"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YEz4GemmU6Ci"},"outputs":[],"source":["# %load_ext tensorboard\n","# %tensorboard --logdir=runs"]},{"cell_type":"markdown","metadata":{"id":"7Gor5Y_RfNS2"},"source":["## 7. 모델 예측"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EwvI6s0wPapW"},"outputs":[],"source":["# testloader에서 미니 배치 가져오기\n","test_iter = iter(testloader)\n","images, labels = next(test_iter)\n","images, labels = images.to(device), labels.to(device)\n","print(images.size(), labels.size())\n","\n","# random한 index로 이미지 한장 준비하기\n","rnd_idx = 1\n","print(images[rnd_idx].shape, labels[rnd_idx])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4tJ7p0n6qkTm"},"outputs":[],"source":["images[rnd_idx].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EsQqEisxGYTI"},"outputs":[],"source":["# not Flatten!\n","# flattend_img = images[rnd_idx].view(1, 784)\n","\n","# 준비된 이미지로 예측하기\n","model.eval()\n","with torch.no_grad():\n","  logit = model(images[rnd_idx].unsqueeze(0)) # model.forward()에서는 배치가 적용된 4차원 입력 기대\n","\n","pred = logit.max(dim=1)[1]\n","print(pred == labels[rnd_idx]) # True : 잘 예측"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lf4crmA-Icxx"},"outputs":[],"source":["print(\"pred:\", pred, \"labels:\", labels[rnd_idx])\n","print(labels_map[pred.cpu().item()], labels_map[labels[rnd_idx].cpu().item()])\n","plt.imshow(images[rnd_idx].permute(1, 2, 0).cpu())"]},{"cell_type":"markdown","metadata":{"id":"BSkUi8toIprR"},"source":["## 8. 모델 평가"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XwzQxaKDJdq4"},"outputs":[],"source":["def evaluation(model, testloader, loss_fn):\n","  total = 0   \n","  correct = 0\n","  test_loss = 0\n","  test_accuracy = 0\n","\n","  # 전방향 예측을 구할 때는 gradient가 필요가 없음음\n","  with torch.no_grad():\n","    for images, labels in testloader: # 이터레이터로부터 next()가 호출되며 미니배치를 반환(images, labels)\n","      # 0. Data를 GPU로 보내기\n","      images, labels = images.to(device), labels.to(device)\n","\n","      # 1. 입력 데이터 준비\n","      # not Flatten\n","      # images.resize_(images.size()[0], 784)\n","      \n","      # 2. 전방향(Forward) 예측\n","      logit = model(images) # 예측 점수\n","      _, preds = torch.max(logit, 1) # 배치에 대한 최종 예측\n","      # preds = logit.max(dim=1)[1] \n","      correct += int((preds == labels).sum()) # 배치치 중 맞은 것의 개수가 correct에 누적\n","      total += labels.shape[0] # 배치 사이즈만큼씩 total에 누적\n","\n","      loss = loss_fn(logit, labels)\n","      test_loss += loss.item() # tensor에서 값을 꺼내와서, 배치의 loss 평균값을 valid_loss에 누적\n","\n","    test_accuracy = correct / total\n","   \n","  print('Test Loss : {:.3f}'.format(test_loss/len(testloader)), \n","        'Test Accuracy : {:.3f}'.format(test_accuracy))\n","\n","model.eval()\n","evaluation(model, testloader, loss_fn)  "]},{"cell_type":"markdown","metadata":{"id":"2tMZR0m-eiy4"},"source":["## 9. 모델 저장"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kjPK_VKsdI0G"},"outputs":[],"source":["# 모델을 저장하는 이유\n","# 1. 예측을 할 때마다 훈련시키는것은 비효율적\n","# 2. 기존 훈련 결과에 이어서 학습을 하고자 할 때\n","\n","# 파이토치에서 모델 저장하기\n","# https://pytorch.org/tutorials/beginner/saving_loading_models.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpYRFzngdkQQ"},"outputs":[],"source":["# 현재 모델에 저장되어 있는 모델 파라미터터\n","# model.state_dict().keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rfG-YsrAdzM3"},"outputs":[],"source":["torch.save(model.state_dict(), 'last_checkpoint.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPtyU6e_elZU"},"outputs":[],"source":["# 시간이 흐른뒤 다시 모델 가져오기\n","last_state_dict = torch.load('last_checkpoint.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aU_BVeHne168"},"outputs":[],"source":["# last_state_dict.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PETgjZf3e3Kr"},"outputs":[],"source":["# 읽어들인 모델 파라미터는 모델 아키텍처에 연결을 시켜줘야 함\n","# load_state_dict() 사용\n","last_model = model\n","last_model.to(device)\n","last_model.load_state_dict(last_state_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54H6Fny5fTFk"},"outputs":[],"source":["last_model.eval()\n","evaluation(last_model, testloader, loss_fn)  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5GtKgu4Bfv8N"},"outputs":[],"source":["# valid loss or accuracy 기준 best model\n","best_state_dict = torch.load('best_checkpoint.pth')\n","best_model = model\n","best_model.to(device)\n","best_model.load_state_dict(best_state_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aM9vhUn2jDo4"},"outputs":[],"source":["best_model.eval()\n","evaluation(best_model, testloader, loss_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NZF0kP2zqUk2"},"outputs":[],"source":["#best_state_dict['conv_block1.0.weight']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ojxZutnDz92b"},"outputs":[],"source":["#last_state_dict['conv_block1.0.weight']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XFc3PNyJmqRq"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1u680xjZR6et3RP_TyZxofpMDUWuNhQ0C","timestamp":1681696705163},{"file_id":"18RkdjRnD4d1BSw5WAPz468ZoreEJtqWN","timestamp":1681276004064},{"file_id":"1ynbg6zrcH2LFyCc8aXpNRf-qSla_sS1n","timestamp":1681263979612}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}